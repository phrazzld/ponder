# TODO: Conversational Journal Assistant (2025 LLM-Native Approach)

## Context & Philosophy

**Original Plan (DEPRECATED)**: Build sentiment classifiers, topic extractors, statistical pattern detection
**Ultrathink Verdict**: Overengineered. We're treating 2025 LLMs like 2016 ML models.

**New Approach**: Conversational interface with Chain-of-Thought reasoning
- Let LLMs reason naturally about journal content
- Extend existing RAG pipeline (ops/ask.rs) rather than rebuild
- Use native Ollama streaming (no wrapper modules needed)
- Store insights as entries in existing embeddings table (no new tables)

**Key Insight**: We already have 80% of what we need. Just add conversation loop.

---

## Completed Work (Keep & Build Upon)

‚úÖ **Phase 1: Foundation**
- AIModels config with env var loading (src/config/mod.rs)
- Model name constants (src/constants.rs)
- Database schema for summaries and patterns (src/db/schema.rs)
- CRUD operations (src/db/summaries.rs, src/db/patterns.rs)
- AI client enhancements (sentiment analysis, topic extraction methods)

‚úÖ **Phase 2: Progressive Summaries**
- Daily/weekly/monthly summary generation (src/ops/summarize.rs)
- Summarize and Summaries CLI subcommands
- Command handlers (cmd_summarize, cmd_summaries)

‚úÖ **Phase 3: Pattern Detection (Partial)**
- Temporal pattern detection (src/ops/patterns.rs)
- Topic clustering via embeddings (src/ops/patterns.rs)

---

## Phase 3: Deprecate 2016-Style Classifiers

**DECISION**: Stop building statistical pattern analyzers. LLMs can reason about patterns naturally when asked conversationally.

### Tasks to SKIP (moved to BACKLOG.md)

~~- [ ] Implement sentiment trend analysis in patterns.rs~~
~~- [ ] Implement correlation discovery in patterns.rs~~
~~- [ ] Add Analyze subcommand for pattern types~~
~~- [ ] Add Patterns subcommand for viewing~~
~~- [ ] Add Insights subcommand for AI narrative~~
~~- [ ] Implement cmd_analyze, cmd_patterns, cmd_insights handlers~~

**Rationale**: These duplicate what conversational interface will do better:
- User: "Do you notice any emotional patterns in my entries?"
- AI: "Let me think through this step-by-step... [CoT reasoning about sentiment trends]"

---

## Phase 4: Conversational Interface (Minimal Implementation)

### Core Conversation Loop

- [x] Add Chain-of-Thought system prompt to src/ai/prompts.rs
  ```
  Files: src/ai/prompts.rs
  Add: COT_SYSTEM_PROMPT constant with step-by-step reasoning instructions
  Format: "Think step-by-step: First... Second... This suggests..."
  Success criteria: Prompt encourages reasoning visibility, cites entries, acknowledges uncertainty
  Pattern: Similar to existing SYSTEM_PROMPT but adds explicit CoT instructions
  ~20 lines
  ```

- [x] Create conversational operation in src/ops/converse.rs
  ```
  Files: NEW src/ops/converse.rs
  Function: start_conversation(db, session, ai_client, config)
  Logic:
    1. Interactive loop reading user input
    2. Assemble context using existing ops::ask::get_relevant_context() pattern
    3. Build CoT prompt with context + user question
    4. Stream response using ai_client.chat() with native streaming
    5. Print chunks as they arrive
    6. Maintain message history in-memory (Vec<Message>) for conversation continuity
    7. Exit on "quit", "exit", or empty input
  Success criteria:
    - User can chat interactively with AI about their journal
    - AI responses cite specific entries
    - AI shows step-by-step reasoning
    - Context assembled from existing RAG pipeline
    - No new database tables needed
  Error handling: Session timeout, Ollama unreachable, decryption failure
  Testing: Integration test with mock input/output, verify context assembly
  ~341 lines (includes helper function and tests)
  Work Log:
  - Implemented start_conversation() with interactive loop
  - Included assemble_conversation_context() helper (next task) in same file
  - Non-streaming for MVP (can add --stream flag later)
  - Conversation history pruned to 20 messages to prevent context overflow
  - Unit tests for context assembly (no entries, limit validation)
  ```

- [x] Add helper function to ops/converse.rs for context assembly
  ```
  Files: src/ops/converse.rs
  Function: assemble_conversation_context(db, session, query, limit) -> AppResult<Vec<(NaiveDate, String)>>
  Note: Completed as part of previous task (included in same file)
  ```

### CLI Integration

- [x] Add Converse subcommand to src/cli/mod.rs
  ```
  Files: src/cli/mod.rs
  Status: COMPLETE - Converse variant exists (line 79), ConverseArgs struct (lines 225-231)
  Work Log:
  - Already implemented with --no-context flag
  - Follows existing subcommand patterns
  ```

- [x] Implement cmd_converse handler in src/main.rs
  ```
  Files: src/main.rs
  Status: COMPLETE - cmd_converse() exists (line 608)
  Work Log:
  - Orchestrates SessionManager ‚Üí Database ‚Üí OllamaClient
  - Calls ops::converse::start_conversation()
  - Error handling for Ollama connection, session timeout
  ```

- [x] Update ops/mod.rs to export converse module
  ```
  Files: src/ops/mod.rs
  Status: COMPLETE - Module declared (line 9), re-exported (line 23)
  ```

### Testing

- [x] Add unit tests for conversation context assembly
  ```
  Files: src/ops/converse.rs (test module)
  Tests:
    - test_assemble_context_no_entries: Empty DB returns empty context [DONE]
    - test_assemble_context_finds_relevant: Vector search returns matching entries [DONE]
    - test_assemble_context_decrypts: Encrypted entries properly decrypted [DONE]
    - test_assemble_context_limits_results: Respects limit parameter [DONE]
  Pattern: Use tempfile + in-memory DB like existing ops tests
  Completed: 284 lines (3 tests marked #[ignore = "requires Ollama"])
  Commit: 2eadad2
  ```

- [x] Add integration test for conversation loop
  ```
  Files: tests/ops_integration_tests.rs
  Test: test_conversation_context_assembly_integration
  Setup:
    - Create temp journal with 5 entries (presentation narrative)
    - Generate embeddings via Ollama
  Verify:
    - Context assembled from relevant entries across 5 queries
    - Semantic relevance maintained
    - Limit enforcement works
    - Dates from relevant entries returned
  Requires: Ollama running locally (marked #[ignore])
  Completed: 189 lines
  Commit: 47341aa
  ```

### Documentation

- [x] Update CLAUDE.md with conversational interface examples
  ```
  Files: CLAUDE.md
  Section: "Conversational Operations (v2.1)"
  Added:
    - Philosophy: 2025 LLMs vs 2016 classifiers
    - High-level flow (5-step process)
    - Example conversation with CoT reasoning
    - Extends existing RAG explanation
    - Implementation details (no new tables, no wrappers)
    - Design rationale with ultrathink references
  Also updated module flow diagram with cmd_converse
  Completed: 81 lines
  Commit: c346076
  ```

---

## Implementation Notes

### What We're NOT Building (Per Ultrathink)

‚ùå **streaming.rs module**: Use Ollama's native `stream=True` parameter directly
‚ùå **context.rs module**: Context assembly logic lives in converse.rs (~80 lines)
‚ùå **db/memory.rs module**: No new table needed - use existing embeddings
‚ùå **conversation_memory table**: Insights stored as regular entries if needed
‚ùå **Session management**: Use existing 30-min timeout, no new session concept
‚ùå **Complex context windowing**: Start simple, optimize if needed

### Reusing Existing Infrastructure

‚úÖ **Vector search**: `db::embeddings::search_similar_chunks()` - already deep module
‚úÖ **RAG pipeline**: `ops::ask.rs` patterns for context assembly
‚úÖ **Decryption**: `crypto::age::decrypt_with_passphrase()` - secure and tested
‚úÖ **Streaming**: Ollama client library native support (no wrapper needed)
‚úÖ **Entry storage**: Existing `entries` table + `embeddings` table

### Design Principles Applied

- **Module depth**: Converse.rs is deep (simple interface: `start_conversation()`, complex orchestration inside)
- **Information hiding**: Streaming, context selection, decryption all hidden from CLI layer
- **Minimal new modules**: 1 new file (converse.rs), 0 new tables, 0 wrapper modules
- **Extend don't rebuild**: Reuses 80% of existing RAG infrastructure

---

## Timeline Estimate

- **Phase 4 (Conversational Interface)**: 1 week
  - Core loop: 2 days
  - CLI integration: 1 day
  - Testing: 2 days
  - Documentation: 0.5 days

**Total for minimal conversational interface: ~5 days**

Compare to original plan: 4 weeks ‚Üí 1 week (80% reduction via simplification)

---

## Success Criteria for Phase 4

When this phase is complete:

1. ‚úÖ Users can run `ponder converse` to start interactive chat
2. ‚úÖ AI responds with step-by-step reasoning (Chain-of-Thought)
3. ‚úÖ AI cites specific journal entries in responses (via context assembly)
4. ‚úÖ Conversation maintains context across multiple turns (in-memory history)
5. ‚ö†Ô∏è  Responses stream in real-time ‚Üí Non-streaming for MVP (can add later)
6. ‚úÖ Works with existing journal entries (no migration needed)
7. ‚úÖ No new database tables required (uses existing embeddings)
8. ‚úÖ Graceful error handling (Ollama down, session timeout, context errors)

---

---

## Phase 5: Intent-Aware Context Retrieval (3-Phase Workflow)

### Context & Rationale

**Problem**: Conversational interface retrieves context for ALL queries, including meta-questions ("what do you think?") that don't need journal context. This wastes time and pollutes responses with irrelevant entries.

**Ultrathink Verdict**: Use LLM-driven reflection phase with structured JSON output (NOT tool calling - that's over-engineered for 2 tools). Three-phase workflow:
1. **Reflection**: LLM decides search vs. respond directly
2. **Conditional Execution**: Fetch context only if needed
3. **Response**: Stream answer with/without context

**Key Design Decision**: Structured JSON ‚Üí Rust enum, reuse existing TemporalConstraint type. Simpler than tool calling, fewer failure modes, faster.

### Cleanup Work

- [x] Revert tool calling infrastructure from ollama.rs
  ```
  Files: src/ai/ollama.rs
  Status: COMPLETE - Commit c85c9fb
  Work Log:
  - Removed 6 tool-related structs
  - Removed chat_with_tools() method
  - Cleaned up orphaned doc comments
  - All tests pass, build clean
  - Pre-commit hooks pass
  ```

### Core Types

- [x] Create ReflectionDecision enum in ops/converse.rs
  ```
  Status: COMPLETE - Commit b8283fc
  Work Log:
  - Added ReflectionDecision enum with Search/Respond variants
  - Search variant includes optional temporal_constraint (defaults to None)
  - Both variants include reasoning field for transparency
  - Reuses TemporalConstraint from query_analysis module
  - Added Default impl for TemporalConstraint (defaults to None)
  ```

- [x] Add reflection system prompt constant
  ```
  Status: COMPLETE - Commit b8283fc (batched with enum)
  Work Log:
  - Added REFLECTION_SYSTEM_PROMPT constant (28 lines)
  - Clear decision criteria for search vs respond
  - Includes temporal constraint examples
  - JSON format specification
  - Example queries for both actions
  ```

### Phase 1: Reflection (Decision Making)

- [x] Implement reflection phase in conversation loop
  ```
  Status: COMPLETE - Commit 6c582a1
  Work Log:
  - Implemented three-phase workflow (Reflection + Conditional Execution)
  - Phase 1: Build reflection messages with last 3 turns, call chat_with_json_format()
  - Phase 2: Match on ReflectionDecision enum, conditional context assembly
  - Graceful error handling: parse errors ‚Üí fallback to Search with warnings
  - User-visible reasoning: "üí≠ Searching/Responding: [reason]"
  - Reuses existing context assembly (temporal filter not yet integrated)
  - ~70 lines added to conversation loop
  ```

### Phase 2: Conditional Execution

- [x] Implement conditional context assembly
  ```
  Status: COMPLETE - Commit 6c582a1 (implemented with reflection phase)
  Work Log:
  - Match expression on ReflectionDecision enum
  - Search ‚Üí calls assemble_conversation_context()
  - Respond ‚Üí returns Vec::new() (no DB query)
  - Error handling preserved (fallback to empty context on error)
  - Temporal constraint captured but not yet passed to context assembly (next task)
  ```

- [~] Update assemble_conversation_context to accept optional temporal constraint
  ```
  Files: src/ops/converse.rs (function signature and implementation)
  Location: Lines 215-280 (existing function)
  Changes:
    1. Add parameter: temporal_constraint: Option<TemporalConstraint>
    2. If Some(constraint), apply date filtering before/after vector search:
       - constraint.to_date_range(today()) ‚Üí (start, end)
       - Filter entry dates to be within range
    3. If None, search all dates (current behavior)
  Success criteria:
    - Temporal constraint correctly filters entries by date
    - None constraint searches all dates (backward compatible)
    - Existing tests still pass
  Error handling: Invalid date range ‚Üí log warning, proceed without filter
  Testing: Add unit test with relative/absolute/none constraints
  ~20 lines modified + 30 lines new test
  ```

### Phase 3: Response Generation (Already Implemented)

- [ ] Verify streaming response works with new workflow
  ```
  Files: src/ops/converse.rs (lines 167-210)
  Verification: Existing streaming code (Phase 3) unchanged
  Success criteria:
    - Responses stream word-by-word regardless of context presence
    - Empty context (Respond action) still generates coherent answers
    - Context (Search action) properly integrated into prompt
  Testing: Manual QA with both action types
  Notes: No code changes needed - just verification
  ```

### Testing

- [ ] Unit tests for ReflectionDecision deserialization
  ```
  Files: src/ops/converse.rs (test module)
  Tests:
    - test_reflection_decision_search_with_relative: Valid search JSON with days_ago
    - test_reflection_decision_search_with_none: Valid search JSON without temporal constraint
    - test_reflection_decision_respond: Valid respond JSON
    - test_reflection_decision_invalid_action: Invalid action type errors gracefully
    - test_reflection_decision_missing_reasoning: Missing required field errors
  Success criteria: All valid JSON deserializes, invalid JSON returns clear errors
  Pattern: Similar to test_query_analysis_json_* tests in query_analysis.rs
  ~60 lines (5 tests)
  ```

- [ ] Integration test for meta-question workflow
  ```
  Files: tests/ops_integration_tests.rs
  Test: test_conversation_meta_question_skips_retrieval
  Setup:
    - Create temp journal with entries
    - Generate embeddings
  Test flow:
    1. Query: "what do you think about this?" (meta-question)
    2. Verify: ReflectionDecision::Respond returned
    3. Verify: No DB context query executed (empty context)
    4. Verify: Response generated from conversation history only
  Success criteria:
    - Meta-questions don't trigger DB queries
    - Responses still coherent without context
    - Reasoning displayed to user
  Requires: Ollama running (mark #[ignore])
  ~80 lines
  ```

- [ ] Integration test for temporal-filtered search
  ```
  Files: tests/ops_integration_tests.rs
  Test: test_conversation_search_with_temporal_filter
  Setup:
    - Create entries across 3 months (Jan, Feb, March)
    - Generate embeddings for all
  Test flow:
    1. Query: "what did I do last month?" (assuming today is March)
    2. Verify: ReflectionDecision::Search with relative constraint (30 days)
    3. Verify: Context only includes February entries, excludes Jan/March
    4. Verify: Response cites only February entries
  Success criteria:
    - Temporal filtering works correctly
    - LLM extracts correct temporal constraint
    - Context assembly respects date range
  Requires: Ollama running (mark #[ignore])
  ~100 lines
  ```

### Documentation

- [ ] Update CLAUDE.md with three-phase workflow explanation
  ```
  Files: CLAUDE.md
  Section: Update "Conversational Operations (v2.1)" section
  Add:
    - Subsection: "Intent-Aware Context Retrieval (v2.2)"
    - Three-phase workflow diagram
    - Example: Meta-question vs. factual query handling
    - ReflectionDecision enum explanation
    - Design rationale (why not tool calling)
  Success criteria: Developers understand workflow, design decisions clear
  Pattern: Similar to existing RAG pipeline explanation
  ~50 lines
  ```

- [ ] Add inline documentation for reflection phase
  ```
  Files: src/ops/converse.rs
  Add:
    - Module-level doc comment explaining three phases
    - Doc comments on ReflectionDecision enum variants
    - Doc comment on REFLECTION_SYSTEM_PROMPT explaining criteria
    - Inline comments in reflection logic explaining flow
  Success criteria: Code self-documents workflow, easy to understand
  Pattern: Follow existing ops module doc style
  ~30 lines total
  ```

---

## Next Steps After Completion

See `BACKLOG.md` for future enhancements:
- Persistent insights storage
- Automatic insight accumulation
- Multi-session context tracking
- Advanced context windowing strategies
- Insight review and curation features
